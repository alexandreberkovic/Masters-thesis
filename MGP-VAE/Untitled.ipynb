{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614ef109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains networks for all individual components of MGP VAE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from flags import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477294a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_FRAMES = 14\n",
    "NDIM = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6072d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(shape=(BATCH_SIZE, NUM_FRAMES, NDIM))\n",
    "mask[:,0,:] = 0\n",
    "mk = torch.from_numpy(mask).float()#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0813664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_triangular(vector):\t\n",
    "\tif (len(vector.size()) == 1):\n",
    "\t\tl = vector.size()[0]\n",
    "\t\tn = int(np.sqrt(0.25 + 2. *l) - 0.5)\n",
    "\t\trev_vec = torch.flip(vector, dims=[0])\n",
    "\t\trev_vec = rev_vec[n:]\n",
    "\t\tvec = torch.cat([vector, rev_vec], 0)\n",
    "\t\ty = vec.view(n,n)\n",
    "\t\ty = torch.transpose(torch.triu(y), dim0=0, dim1=1)\n",
    "\t\treturn y\n",
    "\n",
    "\telif (len(vector.size()) == 2):\n",
    "\t\tl = vector.size()[1]\n",
    "\t\tn = int(np.sqrt(0.25 + 2. *l) - 0.5)\n",
    "\t\trev_vec = torch.flip(vector, dims=[1])\n",
    "\t\trev_vec = rev_vec[:, n:]\n",
    "\t\tvec = torch.cat([vector, rev_vec], 1)\n",
    "\t\ty = vec.view(vector.size()[0], n,n)\n",
    "\t\ty = torch.transpose(torch.triu(y), dim0=1, dim1=2)\n",
    "\t\treturn y\n",
    "\n",
    "\telif (len(vector.size()) == 3):\n",
    "\t\tvector = vector.view(vector.size()[0]*vector.size()[1], vector.size()[2])\n",
    "\t\tl = vector.size()[1]\n",
    "\t\tn = int(np.sqrt(0.25 + 2. *l) - 0.5)\n",
    "\t\trev_vec = torch.flip(vector, dims=[1])\n",
    "\t\trev_vec = rev_vec[:, n:]\n",
    "\t\tvec = torch.cat([vector, rev_vec], 1)\n",
    "\t\ty = vec.view(vector.size()[0], n,n)\n",
    "\t\ty = torch.transpose(torch.triu(y), dim0=1, dim1=2)\n",
    "\t\ty = y.view(BATCH_SIZE, (y.size()[0])//BATCH_SIZE, y.size()[1], y.size()[2])\n",
    "\t\treturn y\n",
    "\n",
    "\telse:\n",
    "\t\traise Exception(\"ERROR in fill_triangular function, size of input is : {}\".format(str(len(vector.size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_matSum = np.ones(NUM_FRAMES * (NUM_FRAMES + 1) // 2)\n",
    "matSum = torch.from_numpy(raw_matSum).float().cuda()\n",
    "matSum = fill_triangular(matSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee928c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class My_Tanh(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(My_Tanh, self).__init__()\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn (0.5 * (self.tanh(x) + 1))\n",
    "\n",
    "def matrix_diag_4d(diagonal):\n",
    "\tdiagonal = diagonal.view(diagonal.size()[0]*diagonal.size()[1], diagonal.size()[2], diagonal.size()[3])\n",
    "\tresult = torch.diagonal(diagonal, dim1 = -2, dim2 = -1)\n",
    "\n",
    "\tresult = result.view(BATCH_SIZE, NDIM, NUM_FRAMES)\n",
    "\treturn result\n",
    "\n",
    "def matrix_diag_3d(diagonal):\n",
    "\tresult = torch.diagonal(diagonal, dim1 = -2, dim2 = -1)\n",
    "\treturn result\n",
    "\n",
    "def create_path(K_L1, mu_L1, BATCH_SIZE=BATCH_SIZE):\n",
    "\t# this function samples random paths from given GP using lower triangular matrices K_L (obtained from covariance matrices) and mean mu_L\n",
    "\t\n",
    "\tinc_L1 = torch.randn(BATCH_SIZE, NUM_FRAMES, NDIM).cuda()\n",
    "\tX1 = torch.einsum('ikj,ijlk->ilj', inc_L1, K_L1) + mu_L1\t# shape = (BATCH_SIZE, NUM_FRAMES, NDIM)\n",
    "\treturn X1\n",
    "\n",
    "def create_path_rho(K_L1, mu_L1, BATCH_SIZE=BATCH_SIZE, rho=0.5):\n",
    "\t# this function samples random paths from given GP using lower triangular matrices K_L (obtained from covariance matrices) and mean mu_L\n",
    "\n",
    "\tc11 = torch.randn(BATCH_SIZE, NUM_FRAMES).cuda()\n",
    "\tc12 = (rho * c11) + (np.sqrt(1.0 - (rho * rho)) * torch.randn(BATCH_SIZE, NUM_FRAMES).cuda())\n",
    "\tc21 = torch.randn(BATCH_SIZE, NUM_FRAMES).cuda()\n",
    "\tc22 = (rho * c21) + (np.sqrt(1.0 - (rho * rho)) * torch.randn(BATCH_SIZE, NUM_FRAMES).cuda())\n",
    "\n",
    "\tinc_L1 = torch.stack([c11, c12, c21, c22], dim=2)\n",
    "\tX1 = torch.einsum('ikj,ijlk->ilj', inc_L1, K_L1) + mu_L1  # shape = (BATCH_SIZE, NUM_FRAMES, NDIM)\n",
    "\n",
    "\treturn X1\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "\t\tself.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn4 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.conv5 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn5 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.conv6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn6 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.conv7 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn7 = nn.BatchNorm2d(num_features=128)\n",
    "\t\tself.conv8 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn8 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "\t\t# layers for MLP\n",
    "\t\tif (H == 64):\n",
    "\t\t\tself.dense1 = nn.Linear(in_features=NUM_FRAMES*128*4*4, out_features=128) \n",
    "\t\telif (H == 32):\n",
    "\t\t\tself.dense1 = nn.Linear(in_features=NUM_FRAMES*128*2*2, out_features=128) \n",
    "\t\tself.bn1_mlp = nn.BatchNorm1d(num_features=128)\n",
    "\t\t\n",
    "\t\tself.raw_kl1_size = NDIM * NUM_FRAMES * NUM_FRAMES\n",
    "\t\tself.dense2_1 = nn.Linear(in_features=128, out_features = self.raw_kl1_size)\n",
    "\t\tself.bn2_1 = nn.BatchNorm1d(num_features=self.raw_kl1_size)\n",
    "\n",
    "\t\tself.dense2_2 = nn.Linear(in_features=128, out_features=(NDIM * NDIM) * ((NDIM * NDIM) + 1) // 2)\n",
    "\t\tself.bn2_2 = nn.BatchNorm1d(num_features=(NDIM * NDIM) * ((NDIM * NDIM) + 1) // 2)\n",
    "\t\tself.dense2_3 = nn.Linear(in_features=128, out_features=NDIM * NUM_FRAMES)\n",
    "\t\tself.bn2_3 = nn.BatchNorm1d(num_features=NDIM * NUM_FRAMES)\n",
    "\t\tself.dense2_4 = nn.Linear(in_features=128, out_features=NDIM * NDIM)\n",
    "\t\tself.bn2_4 = nn.BatchNorm1d(num_features=NDIM * NDIM)\n",
    "\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.elu = nn.ELU()\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\n",
    "\tdef MLP_1(self, x):\n",
    "\t\tx = self.bn1_mlp(self.elu(self.dense1(x)))\n",
    "\t\tx = self.bn2_1(self.tanh(self.dense2_1(x)))\n",
    "\t\treturn x\n",
    "\n",
    "\tdef MLP_3(self, x):\n",
    "\t\tx = self.bn1_mlp(self.elu(self.dense1(x)))\n",
    "\t\tx = self.bn2_3(self.tanh(self.dense2_3(x)))\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, x, BATCH_SIZE=BATCH_SIZE):\n",
    "\t\tx = x.view(BATCH_SIZE * NUM_FRAMES, NUM_INPUT_CHANNELS, H, W)\n",
    "\t\tx = self.bn1(self.elu(self.conv1(x)))\n",
    "\t\tx = self.bn2(self.elu(self.conv2(x)))\n",
    "\t\tx = self.bn3(self.elu(self.conv3(x)))\n",
    "\t\tx = self.bn4(self.elu(self.conv4(x)))\n",
    "\t\tx = self.bn5(self.elu(self.conv5(x)))\n",
    "\t\tx = self.bn6(self.elu(self.conv6(x)))\n",
    "\t\tx = self.bn7(self.elu(self.conv7(x)))\n",
    "\t\tx = self.bn8(self.elu(self.conv8(x)))\n",
    "\t\t\n",
    "\t\tx = x.view(BATCH_SIZE, NUM_FRAMES, 128, x.size()[2], x.size()[3])\n",
    "\n",
    "\t\t# flatten\n",
    "\t\tx = x.view(x.size()[0], -1)\n",
    "\n",
    "\t\t# create path sample \t\n",
    "\t\traw_KL1 = self.MLP_1(x).view(-1, NDIM, NUM_FRAMES, NUM_FRAMES)\n",
    "\t\tKL1 = torch.tril(raw_KL1)\n",
    "\t\t\n",
    "\t\tif(BATCH_SIZE==1):\n",
    "\t\t\tKL1_diag = matrix_diag_3d(KL1)\n",
    "\t\telse:\n",
    "\t\t\tKL1_diag = matrix_diag_4d(KL1)\n",
    "\t\tdet_q1 = torch.prod(KL1_diag*KL1_diag, dim=2)\n",
    "\t\t\n",
    "\t\tmuL1 = self.MLP_3(x).view(-1, NUM_FRAMES, NDIM)\n",
    "\t\t\n",
    "\t\tif(KEEP_RHO):\n",
    "\t\t\tX1 = create_path_rho(KL1, muL1, BATCH_SIZE)\n",
    "\t\telse:\n",
    "\t\t\tX1 = create_path(KL1, muL1, BATCH_SIZE)\n",
    "\n",
    "\t\treturn X1, KL1, muL1, det_q1\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\n",
    "\t\tfactor = NDIM\n",
    "\n",
    "\t\tself.dense1 = nn.Linear(in_features=NUM_FRAMES*factor, out_features=NUM_FRAMES*8*8*16, bias=True)\n",
    "\t\tself.bn_dense1 = nn.BatchNorm1d(num_features=NUM_FRAMES*8*8*16)\n",
    "\t\tself.dense2 = nn.Linear(in_features=NUM_FRAMES*8*8*16, out_features=NUM_FRAMES*8*8*64, bias=True)\n",
    "\t\tself.bn_dense2 = nn.BatchNorm1d(num_features=NUM_FRAMES*8*8*64)\n",
    "\t\t\n",
    "\t\tself.conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.conv4 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn4 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.conv5 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bn5 = nn.BatchNorm2d(num_features=16)\n",
    "\t\tself.conv6 = nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "\t\tself.bn6 = nn.BatchNorm2d(num_features=16)\n",
    "\t\tself.conv7 = nn.ConvTranspose2d(in_channels=16, out_channels=NUM_INPUT_CHANNELS, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\tself.my_tanh = My_Tanh()\n",
    "\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.elu = nn.ELU()\n",
    "\n",
    "\tdef forward(self, x1, BATCH_SIZE=BATCH_SIZE):\n",
    "\t\tx = x1\n",
    "\n",
    "\t\t# flatten \n",
    "\t\tx = x.view(BATCH_SIZE, NUM_FRAMES * NDIM)\n",
    "\t\tx = self.bn_dense1(self.elu(self.dense1(x)))\n",
    "\t\tx = self.bn_dense2(self.elu(self.dense2(x)))\n",
    "\t\t\n",
    "\t\tx = x.view(BATCH_SIZE * NUM_FRAMES, 64, 8, 8)\n",
    "\n",
    "\t\tx = self.bn1(self.elu(self.conv1(x)))\n",
    "\t\tx = self.bn2(self.elu(self.conv2(x)))\n",
    "\t\tx = self.bn3(self.elu(self.conv3(x)))\n",
    "\t\tx = self.bn4(self.elu(self.conv4(x)))\n",
    "\t\tx = self.bn5(self.elu(self.conv5(x)))\n",
    "\t\tif (H == 64):\t\n",
    "\t\t\tx = self.bn6(self.elu(self.conv6(x)))\n",
    "\t\t\n",
    "\t\tx = self.my_tanh(self.conv7(x))\n",
    "\t\tx = (255*x).view(BATCH_SIZE, NUM_FRAMES, NUM_INPUT_CHANNELS, H, W)\n",
    "\t\n",
    "\t\treturn x\n",
    "\n",
    "# model for prediction of future frames\n",
    "class Prediction_Model(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Prediction_Model, self).__init__()\n",
    "\n",
    "\t\tself.fc1 = nn.Linear((NUM_FRAMES-1)*(NDIM), 15)\n",
    "\t\tself.fc2 = nn.Linear(15, NDIM)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(x.size()[0], -1)\n",
    "\t\tx = self.relu(self.fc1(x))\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c3588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
