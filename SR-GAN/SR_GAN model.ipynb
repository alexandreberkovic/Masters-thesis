{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14bace7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3504f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21d9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0da09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, hr_shape):\n",
    "        hr_height, hr_width = hr_shape\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        self.lr_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "        self.hr_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img_lr = self.lr_transform(img)\n",
    "        img_hr = self.hr_transform(img)\n",
    "\n",
    "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cfac6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42a3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision.models import vgg19\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f2334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757b71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2eb9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n",
    "\n",
    "        # Residual blocks\n",
    "        res_blocks = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            res_blocks.append(ResidualBlock(64))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "\n",
    "        # Second conv layer post residual blocks\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n",
    "\n",
    "        # Upsampling layers\n",
    "        upsampling = []\n",
    "        for out_features in range(2):\n",
    "            upsampling += [\n",
    "                # nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "                nn.PReLU(),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "\n",
    "        # Final output layer\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d80e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd623d5",
   "metadata": {},
   "source": [
    "# SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a306cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSuper-resolution of CelebA using Generative Adversarial Networks.\\nThe dataset can be downloaded from: https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADIKlz8PR9zr6Y20qbkunrba/Img/img_align_celeba.zip?dl=0\\n(if not available there see if options are listed at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\\nInstrustion on running the script:\\n1. Download the dataset from the provided link\\n2. Save the folder 'img_align_celeba' to '../../data/'\\n4. Run the sript using command 'python3 srgan.py'\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Super-resolution of CelebA using Generative Adversarial Networks.\n",
    "The dataset can be downloaded from: https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADIKlz8PR9zr6Y20qbkunrba/Img/img_align_celeba.zip?dl=0\n",
    "(if not available there see if options are listed at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n",
    "Instrustion on running the script:\n",
    "1. Download the dataset from the provided link\n",
    "2. Save the folder 'img_align_celeba' to '../../data/'\n",
    "4. Run the sript using command 'python3 srgan.py'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from models import *\n",
    "# from datasets import *\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfdac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15c88148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--checkpoint_interval'], dest='checkpoint_interval', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='interval between model checkpoints', metavar=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=100, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"img_align_celeba\", help=\"name of the dataset\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--decay_epoch\", type=int, default=60, help=\"epoch from which to start lr decay\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--hr_height\", type=int, default=256, help=\"high res. image height\")\n",
    "parser.add_argument(\"--hr_width\", type=int, default=256, help=\"high res. image width\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval between saving image samples\")\n",
    "parser.add_argument(\"--checkpoint_interval\", type=int, default=1, help=\"interval between model checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6085d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=8, channels=3, checkpoint_interval=1, dataset_name='img_align_celeba', decay_epoch=60, epoch=0, hr_height=256, hr_width=256, lr=0.0002, n_cpu=8, n_epochs=100, sample_interval=100)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(\"\")\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83bb0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "hr_shape = (opt.hr_height, opt.hr_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cab02a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = GeneratorResNet()\n",
    "discriminator = Discriminator(input_shape=(opt.channels, *hr_shape))\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_content = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e33cf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_content = criterion_content.cuda()\n",
    "\n",
    "if opt.epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5b91c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1a7299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normpath(join(os.getcwd(), path))\n",
    "path = Path('/home/ec2-user/SageMaker/wikiart_binary/train/post_1910s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eedb3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ImageDataset(str(path), hr_shape=hr_shape),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04d19cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/1678] [D loss: 0.466838] [G loss: 1.340378]\n",
      "[Epoch 0/100] [Batch 100/1678] [D loss: 0.012274] [G loss: 0.948510]\n",
      "[Epoch 0/100] [Batch 200/1678] [D loss: 0.006115] [G loss: 1.134939]\n",
      "[Epoch 0/100] [Batch 300/1678] [D loss: 0.003485] [G loss: 1.025518]\n",
      "[Epoch 0/100] [Batch 400/1678] [D loss: 0.002884] [G loss: 1.145759]\n",
      "[Epoch 0/100] [Batch 500/1678] [D loss: 0.002698] [G loss: 1.359190]\n",
      "[Epoch 0/100] [Batch 600/1678] [D loss: 0.008699] [G loss: 0.966061]\n",
      "[Epoch 0/100] [Batch 700/1678] [D loss: 0.008132] [G loss: 1.316556]\n",
      "[Epoch 0/100] [Batch 800/1678] [D loss: 0.003763] [G loss: 1.060851]\n",
      "[Epoch 0/100] [Batch 900/1678] [D loss: 0.001760] [G loss: 1.041322]\n",
      "[Epoch 0/100] [Batch 1000/1678] [D loss: 0.001524] [G loss: 1.141434]\n",
      "[Epoch 0/100] [Batch 1100/1678] [D loss: 0.003823] [G loss: 1.067171]\n",
      "[Epoch 0/100] [Batch 1200/1678] [D loss: 0.005183] [G loss: 0.884664]\n",
      "[Epoch 0/100] [Batch 1300/1678] [D loss: 0.000876] [G loss: 1.050931]\n",
      "[Epoch 0/100] [Batch 1400/1678] [D loss: 0.000783] [G loss: 0.842045]\n",
      "[Epoch 0/100] [Batch 1500/1678] [D loss: 0.001953] [G loss: 1.111428]\n",
      "[Epoch 0/100] [Batch 1600/1678] [D loss: 0.001373] [G loss: 0.915618]\n",
      "[Epoch 1/100] [Batch 0/1678] [D loss: 0.004082] [G loss: 1.024452]\n",
      "[Epoch 1/100] [Batch 100/1678] [D loss: 0.196768] [G loss: 1.095423]\n",
      "[Epoch 1/100] [Batch 200/1678] [D loss: 0.173693] [G loss: 1.058756]\n",
      "[Epoch 1/100] [Batch 300/1678] [D loss: 0.004477] [G loss: 0.955215]\n",
      "[Epoch 1/100] [Batch 400/1678] [D loss: 0.003842] [G loss: 0.993949]\n",
      "[Epoch 1/100] [Batch 500/1678] [D loss: 0.267713] [G loss: 1.065229]\n",
      "[Epoch 1/100] [Batch 600/1678] [D loss: 0.009085] [G loss: 0.963682]\n",
      "[Epoch 1/100] [Batch 700/1678] [D loss: 0.031187] [G loss: 1.062935]\n",
      "[Epoch 1/100] [Batch 800/1678] [D loss: 0.010163] [G loss: 0.917028]\n",
      "[Epoch 1/100] [Batch 900/1678] [D loss: 0.008802] [G loss: 1.029497]\n",
      "[Epoch 1/100] [Batch 1000/1678] [D loss: 0.007873] [G loss: 0.820690]\n",
      "[Epoch 1/100] [Batch 1100/1678] [D loss: 0.001812] [G loss: 0.929856]\n",
      "[Epoch 1/100] [Batch 1200/1678] [D loss: 0.001298] [G loss: 0.851952]\n",
      "[Epoch 1/100] [Batch 1300/1678] [D loss: 0.001149] [G loss: 0.794899]\n",
      "[Epoch 1/100] [Batch 1400/1678] [D loss: 0.002286] [G loss: 0.992534]\n",
      "[Epoch 1/100] [Batch 1500/1678] [D loss: 0.004377] [G loss: 0.844260]\n",
      "[Epoch 1/100] [Batch 1600/1678] [D loss: 0.001141] [G loss: 0.994321]\n",
      "[Epoch 2/100] [Batch 0/1678] [D loss: 0.000410] [G loss: 0.891290]\n",
      "[Epoch 2/100] [Batch 100/1678] [D loss: 0.002069] [G loss: 0.993809]\n",
      "[Epoch 2/100] [Batch 200/1678] [D loss: 0.003729] [G loss: 0.929141]\n",
      "[Epoch 2/100] [Batch 300/1678] [D loss: 0.010251] [G loss: 0.884040]\n",
      "[Epoch 2/100] [Batch 400/1678] [D loss: 0.003880] [G loss: 0.847225]\n",
      "[Epoch 2/100] [Batch 500/1678] [D loss: 0.000745] [G loss: 0.823777]\n",
      "[Epoch 2/100] [Batch 600/1678] [D loss: 0.000602] [G loss: 0.853206]\n",
      "[Epoch 2/100] [Batch 700/1678] [D loss: 0.001195] [G loss: 0.927397]\n",
      "[Epoch 2/100] [Batch 800/1678] [D loss: 0.001281] [G loss: 0.833999]\n",
      "[Epoch 2/100] [Batch 900/1678] [D loss: 0.004676] [G loss: 0.945201]\n",
      "[Epoch 2/100] [Batch 1000/1678] [D loss: 0.003646] [G loss: 0.915834]\n",
      "[Epoch 2/100] [Batch 1100/1678] [D loss: 0.003933] [G loss: 0.854455]\n",
      "[Epoch 2/100] [Batch 1200/1678] [D loss: 0.001117] [G loss: 0.843243]\n",
      "[Epoch 2/100] [Batch 1300/1678] [D loss: 0.084863] [G loss: 0.757628]\n",
      "[Epoch 2/100] [Batch 1400/1678] [D loss: 0.004976] [G loss: 0.843385]\n",
      "[Epoch 2/100] [Batch 1500/1678] [D loss: 0.011175] [G loss: 0.979082]\n",
      "[Epoch 2/100] [Batch 1600/1678] [D loss: 0.002722] [G loss: 0.872021]\n",
      "[Epoch 3/100] [Batch 0/1678] [D loss: 0.004065] [G loss: 0.852574]\n",
      "[Epoch 3/100] [Batch 100/1678] [D loss: 0.002780] [G loss: 1.063846]\n",
      "[Epoch 3/100] [Batch 200/1678] [D loss: 0.001791] [G loss: 0.792726]\n",
      "[Epoch 3/100] [Batch 300/1678] [D loss: 0.001040] [G loss: 0.896073]\n",
      "[Epoch 3/100] [Batch 400/1678] [D loss: 0.001007] [G loss: 0.908753]\n",
      "[Epoch 3/100] [Batch 500/1678] [D loss: 0.002584] [G loss: 0.889648]\n",
      "[Epoch 3/100] [Batch 600/1678] [D loss: 0.002295] [G loss: 0.789581]\n",
      "[Epoch 3/100] [Batch 700/1678] [D loss: 0.001024] [G loss: 0.871852]\n",
      "[Epoch 3/100] [Batch 800/1678] [D loss: 0.000901] [G loss: 0.745827]\n",
      "[Epoch 3/100] [Batch 900/1678] [D loss: 0.001967] [G loss: 1.017958]\n",
      "[Epoch 3/100] [Batch 1000/1678] [D loss: 0.000497] [G loss: 0.904544]\n",
      "[Epoch 3/100] [Batch 1100/1678] [D loss: 0.000273] [G loss: 0.695274]\n",
      "[Epoch 3/100] [Batch 1200/1678] [D loss: 0.000258] [G loss: 0.773350]\n",
      "[Epoch 3/100] [Batch 1300/1678] [D loss: 0.001196] [G loss: 0.768570]\n",
      "[Epoch 3/100] [Batch 1400/1678] [D loss: 0.000900] [G loss: 0.709301]\n",
      "[Epoch 3/100] [Batch 1500/1678] [D loss: 0.001299] [G loss: 0.897966]\n",
      "[Epoch 3/100] [Batch 1600/1678] [D loss: 0.002101] [G loss: 0.923569]\n",
      "[Epoch 4/100] [Batch 0/1678] [D loss: 0.001136] [G loss: 0.772571]\n",
      "[Epoch 4/100] [Batch 100/1678] [D loss: 0.001404] [G loss: 0.741716]\n",
      "[Epoch 4/100] [Batch 200/1678] [D loss: 0.002107] [G loss: 0.858402]\n",
      "[Epoch 4/100] [Batch 300/1678] [D loss: 0.000983] [G loss: 0.833829]\n",
      "[Epoch 4/100] [Batch 400/1678] [D loss: 0.001279] [G loss: 0.831150]\n",
      "[Epoch 4/100] [Batch 500/1678] [D loss: 0.021715] [G loss: 0.838390]\n",
      "[Epoch 4/100] [Batch 600/1678] [D loss: 0.002775] [G loss: 0.738014]\n",
      "[Epoch 4/100] [Batch 700/1678] [D loss: 0.006841] [G loss: 0.745780]\n",
      "[Epoch 4/100] [Batch 800/1678] [D loss: 0.002723] [G loss: 0.891444]\n",
      "[Epoch 4/100] [Batch 900/1678] [D loss: 0.004225] [G loss: 1.083383]\n",
      "[Epoch 4/100] [Batch 1000/1678] [D loss: 0.009867] [G loss: 0.619859]\n",
      "[Epoch 4/100] [Batch 1100/1678] [D loss: 0.001660] [G loss: 0.689396]\n",
      "[Epoch 4/100] [Batch 1200/1678] [D loss: 0.001241] [G loss: 0.763443]\n",
      "[Epoch 4/100] [Batch 1300/1678] [D loss: 0.001403] [G loss: 0.867809]\n",
      "[Epoch 4/100] [Batch 1400/1678] [D loss: 0.002344] [G loss: 0.656924]\n",
      "[Epoch 4/100] [Batch 1500/1678] [D loss: 0.000649] [G loss: 0.764520]\n",
      "[Epoch 4/100] [Batch 1600/1678] [D loss: 0.000977] [G loss: 0.911406]\n",
      "[Epoch 5/100] [Batch 0/1678] [D loss: 0.001933] [G loss: 0.817805]\n",
      "[Epoch 5/100] [Batch 100/1678] [D loss: 0.000559] [G loss: 0.612705]\n",
      "[Epoch 5/100] [Batch 200/1678] [D loss: 0.002519] [G loss: 0.751980]\n",
      "[Epoch 5/100] [Batch 300/1678] [D loss: 0.003924] [G loss: 0.872158]\n",
      "[Epoch 5/100] [Batch 400/1678] [D loss: 0.002718] [G loss: 0.811435]\n",
      "[Epoch 5/100] [Batch 500/1678] [D loss: 0.000813] [G loss: 0.649556]\n",
      "[Epoch 5/100] [Batch 600/1678] [D loss: 0.007564] [G loss: 0.838004]\n",
      "[Epoch 5/100] [Batch 700/1678] [D loss: 0.001036] [G loss: 0.863195]\n",
      "[Epoch 5/100] [Batch 800/1678] [D loss: 0.001157] [G loss: 0.808207]\n",
      "[Epoch 5/100] [Batch 900/1678] [D loss: 0.000334] [G loss: 0.817235]\n",
      "[Epoch 5/100] [Batch 1000/1678] [D loss: 0.000215] [G loss: 0.577473]\n",
      "[Epoch 5/100] [Batch 1100/1678] [D loss: 0.001910] [G loss: 0.756585]\n",
      "[Epoch 5/100] [Batch 1200/1678] [D loss: 0.000939] [G loss: 0.890217]\n",
      "[Epoch 5/100] [Batch 1300/1678] [D loss: 0.000595] [G loss: 0.853231]\n",
      "[Epoch 5/100] [Batch 1400/1678] [D loss: 0.001329] [G loss: 0.785523]\n",
      "[Epoch 5/100] [Batch 1500/1678] [D loss: 0.000271] [G loss: 1.060295]\n",
      "[Epoch 5/100] [Batch 1600/1678] [D loss: 0.001148] [G loss: 0.768778]\n",
      "[Epoch 6/100] [Batch 0/1678] [D loss: 0.000977] [G loss: 0.865448]\n",
      "[Epoch 6/100] [Batch 100/1678] [D loss: 0.000271] [G loss: 0.661399]\n",
      "[Epoch 6/100] [Batch 200/1678] [D loss: 0.006293] [G loss: 0.693455]\n",
      "[Epoch 6/100] [Batch 300/1678] [D loss: 0.000185] [G loss: 0.597209]\n",
      "[Epoch 6/100] [Batch 400/1678] [D loss: 0.000562] [G loss: 0.927110]\n",
      "[Epoch 6/100] [Batch 500/1678] [D loss: 0.004513] [G loss: 0.640551]\n",
      "[Epoch 6/100] [Batch 600/1678] [D loss: 0.000203] [G loss: 0.941471]\n",
      "[Epoch 6/100] [Batch 700/1678] [D loss: 0.000213] [G loss: 0.816195]\n",
      "[Epoch 6/100] [Batch 800/1678] [D loss: 0.000317] [G loss: 0.794501]\n",
      "[Epoch 6/100] [Batch 900/1678] [D loss: 0.000439] [G loss: 0.889230]\n",
      "[Epoch 6/100] [Batch 1000/1678] [D loss: 0.001180] [G loss: 0.868716]\n",
      "[Epoch 6/100] [Batch 1100/1678] [D loss: 0.000416] [G loss: 0.859765]\n",
      "[Epoch 6/100] [Batch 1200/1678] [D loss: 0.000969] [G loss: 0.807313]\n",
      "[Epoch 6/100] [Batch 1300/1678] [D loss: 0.094728] [G loss: 0.778378]\n",
      "[Epoch 6/100] [Batch 1400/1678] [D loss: 0.004580] [G loss: 0.879261]\n",
      "[Epoch 6/100] [Batch 1500/1678] [D loss: 0.002304] [G loss: 0.855892]\n",
      "[Epoch 6/100] [Batch 1600/1678] [D loss: 0.003520] [G loss: 0.814178]\n",
      "[Epoch 7/100] [Batch 0/1678] [D loss: 0.007210] [G loss: 0.780212]\n",
      "[Epoch 7/100] [Batch 100/1678] [D loss: 0.002926] [G loss: 0.832679]\n",
      "[Epoch 7/100] [Batch 200/1678] [D loss: 0.001723] [G loss: 0.734133]\n",
      "[Epoch 7/100] [Batch 300/1678] [D loss: 0.000708] [G loss: 0.741576]\n",
      "[Epoch 7/100] [Batch 400/1678] [D loss: 0.000331] [G loss: 0.821431]\n",
      "[Epoch 7/100] [Batch 500/1678] [D loss: 0.000656] [G loss: 0.703250]\n",
      "[Epoch 7/100] [Batch 600/1678] [D loss: 0.000784] [G loss: 0.822526]\n",
      "[Epoch 7/100] [Batch 700/1678] [D loss: 0.000458] [G loss: 0.695884]\n",
      "[Epoch 7/100] [Batch 800/1678] [D loss: 0.001478] [G loss: 1.051479]\n",
      "[Epoch 7/100] [Batch 900/1678] [D loss: 0.000226] [G loss: 0.795921]\n",
      "[Epoch 7/100] [Batch 1000/1678] [D loss: 0.000208] [G loss: 0.735726]\n",
      "[Epoch 7/100] [Batch 1100/1678] [D loss: 0.000840] [G loss: 0.979432]\n",
      "[Epoch 7/100] [Batch 1200/1678] [D loss: 0.000420] [G loss: 0.934174]\n",
      "[Epoch 7/100] [Batch 1300/1678] [D loss: 0.000384] [G loss: 0.913329]\n",
      "[Epoch 7/100] [Batch 1400/1678] [D loss: 0.000312] [G loss: 0.851720]\n",
      "[Epoch 7/100] [Batch 1500/1678] [D loss: 0.000559] [G loss: 0.718863]\n",
      "[Epoch 7/100] [Batch 1600/1678] [D loss: 0.000511] [G loss: 0.972790]\n",
      "[Epoch 8/100] [Batch 0/1678] [D loss: 0.000161] [G loss: 0.905204]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-0623a45a1199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Configure model input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimgs_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimgs_hr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "\n",
    "        # Configure model input\n",
    "        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
    "        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a high resolution image from low resolution input\n",
    "        gen_hr = generator(imgs_lr)\n",
    "\n",
    "        # Adversarial loss\n",
    "        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
    "\n",
    "        # Content loss\n",
    "        gen_features = feature_extractor(gen_hr)\n",
    "        real_features = feature_extractor(imgs_hr)\n",
    "        loss_content = criterion_content(gen_features, real_features.detach())\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_content + 1e-3 * loss_GAN\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss of real and fake images\n",
    "        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
    "        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "        if i%100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
    "            )\n",
    "#             sys.stdout.write(\n",
    "#                 \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "#                 % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
    "#             )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            # Save image grid with upsampled inputs and SRGAN outputs\n",
    "            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
    "            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n",
    "            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
    "            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n",
    "            save_image(img_grid, \"images/%d.png\" % batches_done, normalize=False)\n",
    "\n",
    "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), \"saved_models/generator_%d.pth\" % epoch)\n",
    "        torch.save(discriminator.state_dict(), \"saved_models/discriminator_%d.pth\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f8a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
