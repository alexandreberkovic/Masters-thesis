{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ae914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generative Adversarial Network for MNIST.\"\"\"\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "N_GPUS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b02643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.layernorm\n",
    "import tflib.save_images\n",
    "# import tflib.imagenet\n",
    "import tflib.plot\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd71be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DATASET = Path('/home/ec2-user/SageMaker/portrait_landscape')\n",
    "\n",
    "DIM_G_64  = 64\n",
    "DIM_G_32  = 128\n",
    "DIM_G_16  = 256\n",
    "DIM_G_8   = 512\n",
    "DIM_G_4   = 512\n",
    "\n",
    "DIM_D_64  = 128\n",
    "DIM_D_32  = 256\n",
    "DIM_D_16  = 512\n",
    "DIM_D_8   = 1024\n",
    "DIM_D_4   = 1024\n",
    "\n",
    "NORMALIZATION_G = True\n",
    "NORMALIZATION_D = True\n",
    "\n",
    "ITERS = 200000\n",
    "LR = 1e-4\n",
    "DECAY = True\n",
    "CRITIC_ITERS = 5\n",
    "MOMENTUM_G = 0.\n",
    "MOMENTUM_D = 0.\n",
    "GEN_BS_MULTIPLE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0058cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratorAndDiscriminator():\n",
    "    return ResnetGenerator, ResnetDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b53a0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 64\n",
      "\tCRITIC_ITERS: 5\n",
      "\tDATASET: /home/ec2-user/SageMaker/portrait_landscape\n",
      "\tDECAY: True\n",
      "\tDEVICES: ['/gpu:0']\n",
      "\tDIM_D_16: 512\n",
      "\tDIM_D_32: 256\n",
      "\tDIM_D_4: 1024\n",
      "\tDIM_D_64: 128\n",
      "\tDIM_D_8: 1024\n",
      "\tDIM_G_16: 256\n",
      "\tDIM_G_32: 128\n",
      "\tDIM_G_4: 512\n",
      "\tDIM_G_64: 64\n",
      "\tDIM_G_8: 512\n",
      "\tGEN_BS_MULTIPLE: 1\n",
      "\tITERS: 200000\n",
      "\tLR: 0.0001\n",
      "\tMOMENTUM_D: 0.0\n",
      "\tMOMENTUM_G: 0.0\n",
      "\tNORMALIZATION_D: True\n",
      "\tNORMALIZATION_G: True\n",
      "\tN_GPUS: 1\n",
      "\tOUTPUT_DIM: 49152\n"
     ]
    }
   ],
   "source": [
    "lib.print_model_settings(locals().copy())\n",
    "\n",
    "OUTPUT_DIM = 3*128*128\n",
    "DEVICES = ['/gpu:{}'.format(i) for i in range(N_GPUS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d4feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinearity(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Normalize(name, inputs):\n",
    "    if ('Discriminator' in name) and NORMALIZATION_D:\n",
    "        return lib.ops.layernorm.Layernorm(name,[1,2,3],inputs)\n",
    "    elif ('Generator' in name) and NORMALIZATION_G:\n",
    "        return lib.ops.batchnorm.Batchnorm(name,[0,2,3],inputs,fused=True)\n",
    "\n",
    "def ConvMeanPool(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=he_init, biases=biases)\n",
    "    output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "    return output\n",
    "\n",
    "def MeanPoolConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases)\n",
    "    return output\n",
    "\n",
    "def ScaledUpsampleConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.concat([output, output, output, output], axis=1)\n",
    "    output = tf.transpose(output, [0,2,3,1])\n",
    "    output = tf.depth_to_space(output, 2)\n",
    "    output = tf.transpose(output, [0,3,1,2])\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases, gain=0.5)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d13b649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualBlock(name, input_dim, output_dim, filter_size, inputs, resample=None):\n",
    "    \"\"\"\n",
    "    resample: None, 'down', or 'up'\n",
    "    \"\"\"\n",
    "    if resample=='down':\n",
    "        conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim, output_dim=input_dim)\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim, output_dim=output_dim, stride=2)\n",
    "        # conv_shortcut = functools.partial(lib.ops.conv2d.Conv2D, stride=2)\n",
    "        # conv_2        = functools.partial(ConvMeanPool, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_shortcut = MeanPoolConv\n",
    "    elif resample=='up':\n",
    "        conv_1        = functools.partial(ScaledUpsampleConv, input_dim=input_dim, output_dim=output_dim)\n",
    "        # conv_1        = functools.partial(lib.ops.deconv2d.Deconv2D, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=output_dim, output_dim=output_dim)\n",
    "        conv_shortcut = ScaledUpsampleConv\n",
    "    elif resample==None:\n",
    "        conv_shortcut = lib.ops.conv2d.Conv2D\n",
    "        conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim,  output_dim=output_dim)\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=output_dim, output_dim=output_dim)\n",
    "    else:\n",
    "        raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim==input_dim and resample==None:\n",
    "        shortcut = inputs # Identity skip-connection\n",
    "    else:\n",
    "        shortcut = inputs\n",
    "        # shortcut = Normalize(name+'.NShortcut', shortcut)\n",
    "        shortcut = conv_shortcut(name+'.Shortcut', input_dim=input_dim, output_dim=output_dim, filter_size=1, he_init=False, biases=True, inputs=shortcut)\n",
    "\n",
    "    output = inputs\n",
    "    output = Normalize(name+'.N1', output)\n",
    "    output = nonlinearity(output)\n",
    "    output = conv_1(name+'.Conv1', filter_size=filter_size, inputs=output)\n",
    "    output = Normalize(name+'.N2', output)\n",
    "    output = nonlinearity(output)\n",
    "    output = conv_2(name+'.Conv2', filter_size=filter_size, inputs=output)\n",
    "    # output = Normalize(name+'.N3', output)\n",
    "    # return output\n",
    "    return shortcut + output\n",
    "    # return 0.7*(shortcut+output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d0a2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResnetGenerator(n_samples, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random_normal([n_samples, 128])\n",
    "\n",
    "    output = lib.ops.linear.Linear('Generator.Input', 128, 4*4*DIM_G_4, noise)\n",
    "    output = tf.reshape(output, [-1, DIM_G_4, 4, 4])\n",
    "\n",
    "    # output = ResidualBlock('Generator.4_1', DIM_G_4, DIM_G_4, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Generator.4_2', DIM_G_4, DIM_G_4, 3, output, resample=None)\n",
    "    output = ResidualBlock('Generator.4_3', DIM_G_4, DIM_G_8, 3, output, resample='up')\n",
    "\n",
    "    # output = ResidualBlock('Generator.8_1', DIM_G_8, DIM_G_8, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Generator.8_2', DIM_G_8, DIM_G_8, 3, output, resample=None)\n",
    "    output = ResidualBlock('Generator.8_3', DIM_G_8, DIM_G_16, 3, output, resample='up')\n",
    "\n",
    "    # output = ResidualBlock('Generator.16_1', DIM_G_16, DIM_G_16, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Generator.16_2', DIM_G_16, DIM_G_16, 3, output, resample=None)\n",
    "    output = ResidualBlock('Generator.16_3', DIM_G_16, DIM_G_32, 3, output, resample='up')\n",
    "\n",
    "    # output = ResidualBlock('Generator.32_1', DIM_G_32, DIM_G_32, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Generator.32_2', DIM_G_32, DIM_G_32, 3, output, resample=None)\n",
    "    output = ResidualBlock('Generator.32_3', DIM_G_32, DIM_G_64, 3, output, resample='up')\n",
    "\n",
    "    output = Normalize('Generator.OutputN', output)\n",
    "    output = nonlinearity(output)\n",
    "    output = ScaledUpsampleConv('Generator.Output', DIM_G_64, 3, 5, output, he_init=False)\n",
    "    # output = lib.ops.deconv2d.Deconv2D('Generator.Output', DIM_G_64, 3, 5, output, he_init=False)\n",
    "\n",
    "    output = tf.tanh(output)\n",
    "\n",
    "    return tf.reshape(output, [-1, OUTPUT_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21da4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResnetDiscriminator(inputs):\n",
    "    output = tf.reshape(inputs, [-1, 3, 128, 128])\n",
    "\n",
    "    output = lib.ops.conv2d.Conv2D('Discriminator.Input', 3, DIM_D_64, 5, output, he_init=True, stride=2)\n",
    "\n",
    "    # output = ResidualBlock('Discriminator.64_1', DIM_D_64, DIM_D_64, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Discriminator.64_2', DIM_D_64, DIM_D_64, 3, output, resample=None)\n",
    "    output = ResidualBlock('Discriminator.64_3', DIM_D_64, DIM_D_32, 3, output, resample='down')\n",
    "\n",
    "    # output = ResidualBlock('Discriminator.32_1', DIM_D_32, DIM_D_32, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Discriminator.32_2', DIM_D_32, DIM_D_32, 3, output, resample=None)\n",
    "    output = ResidualBlock('Discriminator.32_3', DIM_D_32, DIM_D_16, 3, output, resample='down')\n",
    "\n",
    "    # output = ResidualBlock('Discriminator.16_1', DIM_D_16, DIM_D_16, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Discriminator.16_2', DIM_D_16, DIM_D_16, 3, output, resample=None)\n",
    "    output = ResidualBlock('Discriminator.16_3', DIM_D_16, DIM_D_8, 3, output, resample='down')\n",
    "\n",
    "    output = ResidualBlock('Discriminator.8_1', DIM_D_8, DIM_D_8, 3, output, resample=None)\n",
    "    output = ResidualBlock('Discriminator.8_2', DIM_D_8, DIM_D_8, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Discriminator.8_3', DIM_D_8, DIM_D_4, 3, output, resample='down')\n",
    "\n",
    "    # output = ResidualBlock('Discriminator.4_1', DIM_D_4, DIM_D_4, 3, output, resample=None)\n",
    "    # output = ResidualBlock('Discriminator.4_2', DIM_D_4, DIM_D_4, 3, output, resample=None)\n",
    "\n",
    "    # output = Normalize('Discriminator.OutputN', output)\n",
    "    # output = output / 10.\n",
    "    output = tf.reduce_mean(output, axis=[2,3])\n",
    "    output = lib.ops.linear.Linear('Discriminator.Output', DIM_D_8, 1, output)\n",
    "\n",
    "    # output = Normalize('Discriminator.OutputN', output)\n",
    "    # output = nonlinearity(output)\n",
    "    # output = tf.reshape(output, [-1, 4*4*DIM_D_4])\n",
    "    # output = lib.ops.linear.Linear('Discriminator.Output', 4*4*DIM_D_4, 1, output)\n",
    "\n",
    "    return tf.reshape(output, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "187b3539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/Masters-thesis/AC-WGAN/tflib/ops/batchnorm.py:51: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 11 and 10 for 'add' (op: 'AddV2') with input shapes: [64,512,11,11], [64,512,10,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1607\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 11 and 10 for 'add' (op: 'AddV2') with input shapes: [64,512,11,11], [64,512,10,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-a4e4686b47a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mframe_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mfixed_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mfixed_noise_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_noise_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-79ebb607abbe>\u001b[0m in \u001b[0;36mResnetGenerator\u001b[0;34m(n_samples, noise)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# output = ResidualBlock('Generator.4_1', DIM_G_4, DIM_G_4, 3, output, resample=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# output = ResidualBlock('Generator.4_2', DIM_G_4, DIM_G_4, 3, output, resample=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generator.4_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIM_G_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIM_G_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'up'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# output = ResidualBlock('Generator.8_1', DIM_G_8, DIM_G_8, 3, output, resample=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-19710068f343>\u001b[0m in \u001b[0;36mResidualBlock\u001b[0;34m(name, input_dim, output_dim, filter_size, inputs, resample)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# output = Normalize(name+'.N3', output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# return output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mshortcut\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;31m# return 0.7*(shortcut+output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 549\u001b[0;31m         \"AddV2\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    550\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    793\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3355\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3357\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m   def _create_op_internal(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3424\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3426\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3427\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1768\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1769\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1770\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1771\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 11 and 10 for 'add' (op: 'AddV2') with input shapes: [64,512,11,11], [64,512,10,10]."
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "\n",
    "    Generator, Discriminator = GeneratorAndDiscriminator()\n",
    "\n",
    "    iteration = tf.placeholder(tf.int32, shape=None)\n",
    "    all_real_data_conv = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 3, 128, 128])\n",
    "\n",
    "    if (len(DEVICES)%2==0) and (len(DEVICES)>=2):\n",
    "\n",
    "        fake_data_splits = []\n",
    "        for device in DEVICES:\n",
    "            with tf.device(device):\n",
    "                fake_data_splits.append(Generator(BATCH_SIZE/len(DEVICES)))\n",
    "        # fake_data = tf.concat(fake_data_splits, axis=0)\n",
    "        # fake_data_splits = tf.split(fake_data, len(DEVICES))\n",
    "\n",
    "        all_real_data = tf.reshape(2*((tf.cast(all_real_data_conv, tf.float32)/255.)-.5), [BATCH_SIZE, OUTPUT_DIM])\n",
    "        all_real_data_splits = tf.split(all_real_data, len(DEVICES)/2)\n",
    "\n",
    "        DEVICES_B = DEVICES[:len(DEVICES)/2]\n",
    "        DEVICES_A = DEVICES[len(DEVICES)/2:]\n",
    "\n",
    "        disc_costs = []\n",
    "        for i, device in enumerate(DEVICES_A):\n",
    "            with tf.device(device):\n",
    "                real_and_fake_data = lib.concat([all_real_data_splits[i]] + [fake_data_splits[i]] + [fake_data_splits[len(DEVICES_A)+i]], axis=0)\n",
    "                disc_all = Discriminator(real_and_fake_data)\n",
    "                disc_real = disc_all[:BATCH_SIZE/len(DEVICES_A)]\n",
    "                disc_fake = disc_all[BATCH_SIZE/len(DEVICES_A):]\n",
    "                disc_costs.append(tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real))\n",
    "\n",
    "        for i, device in enumerate(DEVICES_B):\n",
    "            with tf.device(device):\n",
    "                real_data = tf.identity(all_real_data_splits[i]) # transfer from gpu0\n",
    "                fake_data__ = lib.concat([fake_data_splits[i], fake_data_splits[len(DEVICES_A)+i]], axis=0)\n",
    "                alpha = tf.random_uniform(\n",
    "                    shape=[BATCH_SIZE/len(DEVICES_A),1], \n",
    "                    minval=0.,\n",
    "                    maxval=1.\n",
    "                )\n",
    "                differences = fake_data__ - real_data\n",
    "                interpolates = real_data + (alpha*differences)\n",
    "                gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "                slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "                # print \"WARNING NO LIPSCHITZ PENALTY\"\n",
    "                gradient_penalty = 10.*tf.reduce_mean((slopes-1.)**2)\n",
    "                disc_costs.append(gradient_penalty)\n",
    "\n",
    "        disc_cost = tf.add_n(disc_costs) / len(DEVICES_A)\n",
    "\n",
    "        if DECAY:\n",
    "            decay = tf.maximum(0., 1.-(tf.cast(iteration, tf.float32)/ITERS))\n",
    "        else:\n",
    "            decay = 1.\n",
    "        disc_train_op = tf.train.AdamOptimizer(learning_rate=LR*decay, beta1=MOMENTUM_D, beta2=0.9).minimize(disc_cost, var_list=lib.params_with_name('Discriminator.'), colocate_gradients_with_ops=True)\n",
    "\n",
    "        gen_costs = []\n",
    "        for device in DEVICES:\n",
    "            with tf.device(device):\n",
    "                gen_costs.append(-tf.reduce_mean(Discriminator(Generator(GEN_BS_MULTIPLE*BATCH_SIZE/len(DEVICES)))))\n",
    "        gen_cost = tf.add_n(gen_costs) / len(DEVICES)\n",
    "        gen_train_op = tf.train.AdamOptimizer(learning_rate=LR*decay, beta1=MOMENTUM_G, beta2=0.9).minimize(gen_cost, var_list=lib.params_with_name('Generator'), colocate_gradients_with_ops=True)\n",
    "\n",
    "\n",
    "#     else:\n",
    "#         raise Exception()\n",
    "        # split_real_data_conv = lib.split(all_real_data_conv, len(DEVICES), axis=0)\n",
    "\n",
    "        # gen_costs, disc_costs = [],[]\n",
    "\n",
    "        # for device_index, (device, real_data_conv) in enumerate(zip(DEVICES, split_real_data_conv)):\n",
    "        #     with tf.device(device):\n",
    "\n",
    "        #         real_data = tf.reshape(2*((tf.cast(real_data_conv, tf.float32)/255.)-.5), [BATCH_SIZE/len(DEVICES), OUTPUT_DIM])\n",
    "        #         fake_data = Generator(BATCH_SIZE/len(DEVICES))\n",
    "\n",
    "        #         disc_all = Discriminator(lib.concat([real_data, fake_data],0))\n",
    "        #         disc_real = disc_all[:tf.shape(real_data)[0]]\n",
    "        #         disc_fake = disc_all[tf.shape(real_data)[0]:]\n",
    "\n",
    "        #         gen_cost = -tf.reduce_mean(Discriminator(Generator(GEN_BS_MULTIPLE*BATCH_SIZE/len(DEVICES))))\n",
    "        #         disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "\n",
    "        #         alpha = tf.random_uniform(\n",
    "        #             shape=[BATCH_SIZE/len(DEVICES),1], \n",
    "        #             minval=0.,\n",
    "        #             maxval=1.\n",
    "        #         )\n",
    "        #         differences = fake_data - real_data\n",
    "        #         interpolates = real_data + (alpha*differences)\n",
    "        #         interpolates = tf.stop_gradient(interpolates)\n",
    "        #         gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "        #         slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        #         lipschitz_penalty = 100.*tf.reduce_mean((slopes-1.)**2)\n",
    "        #         disc_cost += lipschitz_penalty\n",
    "\n",
    "        #         gen_costs.append(gen_cost)\n",
    "        #         disc_costs.append(disc_cost)\n",
    "\n",
    "        # gen_cost = tf.add_n(gen_costs) / len(DEVICES)\n",
    "        # disc_cost = tf.add_n(disc_costs) / len(DEVICES)\n",
    "\n",
    "        # if DECAY:\n",
    "        #     decay = tf.maximum(0., 1.-(tf.cast(iteration, tf.float32)/ITERS))\n",
    "        # else:\n",
    "        #     decay = 1.\n",
    "        # gen_train_op = tf.train.AdamOptimizer(learning_rate=LR*decay, beta1=MOMENTUM_G, beta2=0.9).minimize(gen_cost, var_list=lib.params_with_name('Generator'), colocate_gradients_with_ops=True)\n",
    "        # disc_train_op = tf.train.AdamOptimizer(learning_rate=LR*decay, beta1=MOMENTUM_D, beta2=0.9).minimize(disc_cost, var_list=lib.params_with_name('Discriminator.'), colocate_gradients_with_ops=True)\n",
    "\n",
    "\n",
    "    frame_i = [0]\n",
    "    fixed_noise = tf.constant(np.random.normal(size=(64, 128)).astype('float32'))\n",
    "    fixed_noise_samples = Generator(64, noise=fixed_noise)\n",
    "    def generate_image(frame):\n",
    "        samples = session.run(fixed_noise_samples)\n",
    "        samples = ((samples+1.)*(255.99/2)).astype('int32')\n",
    "        lib.save_images.save_images(samples.reshape((64, 3, 128, 128)), 'samples_{}.png'.format(frame))\n",
    "\n",
    "    if DATASET == 'imagenet':\n",
    "        train_gen = lib.imagenet.load(BATCH_SIZE)\n",
    "\n",
    "    def inf_train_gen():\n",
    "        while True:\n",
    "            for images, in train_gen():\n",
    "                yield images\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "\n",
    "    generate_image(0)\n",
    "\n",
    "    gen = inf_train_gen()\n",
    "\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "    # Uncomment this to restore params\n",
    "    # print \"WARNING RESTORING PARAMS FROM CHECKPOINT\"\n",
    "    # saver.restore(session, os.getcwd()+\"/params.ckpt\")\n",
    "\n",
    "    for _iteration in xrange(ITERS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in xrange(CRITIC_ITERS):\n",
    "            _data = gen.next()\n",
    "            _data = _data.reshape((BATCH_SIZE,3,128,128))\n",
    "            _disc_cost, _ = session.run(\n",
    "                [disc_cost, disc_train_op], \n",
    "                feed_dict={all_real_data_conv: _data, iteration: _iteration}#, fake_data: fake_data_buffer[np.random.choice(BUFFER_LEN*BATCH_SIZE, BATCH_SIZE)]}\n",
    "            )\n",
    "\n",
    "        _ = session.run(\n",
    "            gen_train_op,\n",
    "            feed_dict={iteration: _iteration}\n",
    "        )\n",
    "\n",
    "        lib.plot.plot('cost', _disc_cost)\n",
    "        lib.plot.plot('time', time.time() - start_time)\n",
    "\n",
    "        if _iteration % 100 == 0:\n",
    "            generate_image(_iteration)\n",
    "\n",
    "        if _iteration % 1000 == 0:\n",
    "            saver.save(session, 'params.ckpt')\n",
    "\n",
    "        if _iteration % 5 == 0:\n",
    "            lib.plot.flush(print_stds=True)\n",
    "\n",
    "        lib.plot.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623431d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
