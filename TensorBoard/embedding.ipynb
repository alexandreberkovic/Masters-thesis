{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2748d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.15.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da79b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "\n",
    "LOG_DIR = PATH+ '/embedding-logs'\n",
    "#metadata = os.path.join(LOG_DIR, 'metadata2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e423fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ec2-user/SageMaker/wikiart_styles_224'\n",
    "data_dir_list = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b90a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-Pointillism\n",
      "\n",
      "Loaded the images of dataset-Contemporary_Realism\n",
      "\n",
      "Loaded the images of dataset-High_Renaissance\n",
      "\n",
      "Loaded the images of dataset-Cubism\n",
      "\n",
      "Loaded the images of dataset-Post_Impressionism\n",
      "\n",
      "Loaded the images of dataset-Impressionism\n",
      "\n",
      "Loaded the images of dataset-Fauvism\n",
      "\n",
      "Loaded the images of dataset-Minimalism\n",
      "\n",
      "Loaded the images of dataset-Pop_Art\n",
      "\n",
      "Loaded the images of dataset-Expressionism\n",
      "\n",
      "Loaded the images of dataset-Baroque\n",
      "\n",
      "Loaded the images of dataset-Abstract_Expressionism\n",
      "\n",
      "Loaded the images of dataset-Realism\n",
      "\n",
      "Loaded the images of dataset-Naive_Art_Primitivism\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.96 GiB for an array with shape (56800, 224, 224, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-caa6a4972e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 7.96 GiB for an array with shape (56800, 224, 224, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "img_data=[]\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "        input_img_resize=cv2.resize(input_img,(224,224))\n",
    "        img_data.append(input_img_resize)\n",
    "    \n",
    "                \n",
    "img_data = np.array(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f7f7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fe1305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_vectors_shape: (400, 4096)\n",
      "num of images: 400\n",
      "size of individual feature vector: 4096\n"
     ]
    }
   ],
   "source": [
    "feature_vectors = np.loadtxt('feature_vectors_400_samples.txt')\n",
    "print (\"feature_vectors_shape:\",feature_vectors.shape)\n",
    "print (\"num of images:\",feature_vectors.shape[0])\n",
    "print (\"size of individual feature vector:\",feature_vectors.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7894861",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples=feature_vectors.shape[0]\n",
    "num_of_samples_each_class = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ef5dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.Variable(feature_vectors, name='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d634d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "y[0:100]=0\n",
    "y[100:200]=1\n",
    "y[200:300]=2\n",
    "y[300:]=3\n",
    "\n",
    "names = ['cats','dogs','horses','humans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6a773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5608c872",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-71e4f835f49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import os, argparse\n",
    "import pickle\n",
    "import cv2, spacy, numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.externals import joblib\n",
    "from keras import backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# File paths for the model, all of these except the CNN Weights are \n",
    "# provided in the repo, See the VGG_model/README.md to download VGG weights\n",
    "CNN_weights_file_name   = 'VGG_model/vgg16_weights.h5'\n",
    "\n",
    "\n",
    "# Chagne the value of verbose to 0 to avoid printing the progress statements\n",
    "verbose = 1\n",
    "\n",
    "def get_image_model(CNN_weights_file_name):\n",
    "    ''' Takes the CNN weights file, and returns the VGG model update \n",
    "    with the weights. Requires the file VGG.py inside models/CNN '''\n",
    "    from VGG_model.VGG import VGG_16\n",
    "    image_model = VGG_16(CNN_weights_file_name)\n",
    "\n",
    "    # this is standard VGG 16 without the last two layers\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # one may experiment with \"adam\" optimizer, but the loss function for\n",
    "    # this kind of task is pretty standard\n",
    "    image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    return image_model\n",
    "\n",
    "vgg16_model = get_image_model(CNN_weights_file_name)\n",
    "\n",
    "def get_image_features(image_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the \n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    image_features = np.zeros((1, 4096))\n",
    "    # Magic_Number = 4096  > Comes from last layer of VGG Model\n",
    "\n",
    "    # Since VGG was trained as a image of 224x224, every new image\n",
    "    # is required to go through the same transformation\n",
    "    im = cv2.resize(cv2.imread(image_file_name), (224, 224))\n",
    "\n",
    "\n",
    "    # The mean pixel values are taken from the VGG authors, which are the values computed from the training dataset.\n",
    "    mean_pixel = [103.939, 116.779, 123.68]\n",
    "\n",
    "    im = im.astype(np.float32, copy=False) # shape of im = (224,224,3)\n",
    "    \n",
    "    for c in range(3):\n",
    "        im[:, :, c] = im[:, :, c] - mean_pixel[c]        \n",
    "\n",
    "    im = im.transpose((2,0,1)) # convert the image to RGBA  # shame of im= (3,224,224)\n",
    "\n",
    "    \n",
    "    # this axis dimension is required becuase VGG was trained on a dimension\n",
    "    # of 1, 3, 224, 224 (first axis is for the batch size\n",
    "    # even though we are using only one image, we have to keep the dimensions consistent\n",
    "    im = np.expand_dims(im, axis=0)  # shape of im = (1,3,224,224)\n",
    "\n",
    "    image_features[0,:] = vgg16_model.predict(im)[0]\n",
    "    return image_features\n",
    "\n",
    "\n",
    "PATH=os.getcwd()\n",
    "data_path = PATH + '/data'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "image_features_list=[]\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Extracting Features of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        image_features=get_image_features(data_path + '/'+ dataset + '/'+ img )\n",
    "        image_features_list.append(image_features)\n",
    "    \n",
    "    \n",
    "image_features_arr=np.asarray(image_features_list)\n",
    "image_features_arr = np.rollaxis(image_features_arr,1,0)\n",
    "image_features_arr = image_features_arr[0,:,:]\n",
    "\n",
    "np.savetxt('feature_vectors_400_samples.txt',image_features_arr)\n",
    "#feature_vectors = np.loadtxt('feature_vectors.txt')\n",
    "pickle.dump(image_features_arr, open('feature_vectors_400_samples.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
