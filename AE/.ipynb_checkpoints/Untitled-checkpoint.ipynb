{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da576cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from shutil import copyfile\n",
    "from glob import glob\n",
    "from json import load, dump\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D,\\\n",
    "    Activation\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from os.path import basename\n",
    "from time import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a23869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7952d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_dict():\n",
    "    # Create a new version only including tiny 200 classes\n",
    "    df = pd.read_csv('./tiny-imagenet-200/words.txt', sep='\\t', header=None)\n",
    "    keys, classes = df[0], df[1]\n",
    "    class_dict = dict(zip(keys, classes))\n",
    "\n",
    "    tiny_class_dict = {}\n",
    "    cur_index = 0\n",
    "\n",
    "    for directory in glob('./tiny-imagenet-200/train/*'):\n",
    "        cur_key = basename(directory)\n",
    "        tiny_class_dict[cur_key] = {'class': class_dict[cur_key],\n",
    "                                    'index': cur_index}\n",
    "        cur_index += 1\n",
    "\n",
    "    dump(tiny_class_dict, open('./tiny-imagenet-200/class_dict.json', 'w'),\n",
    "         indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b9a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val_class_dict():\n",
    "    tiny_class_dict = load(open('./tiny-imagenet-200/class_dict.json', 'r'))\n",
    "    tiny_val_class_dict = {}\n",
    "\n",
    "    # Create a dictionary for validation images\n",
    "    df = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t',\n",
    "                     header=None)\n",
    "    image_names = df[0]\n",
    "    image_classes = df[1]\n",
    "\n",
    "    for i in range(len(image_names)):\n",
    "        tiny_val_class_dict[image_names[i]] = {\n",
    "            'class': tiny_class_dict[image_classes[i]]['class'],\n",
    "            'index': tiny_class_dict[image_classes[i]]['index'],\n",
    "        }\n",
    "\n",
    "    dump(tiny_val_class_dict, open('./tiny-imagenet-200/val_class_dict.json',\n",
    "                                   'w'),\n",
    "         indent=2)\n",
    "\n",
    "\n",
    "def split_val_data():\n",
    "    # Split validation images to 50% early stopping and 50% hold-out testing\n",
    "    val_images = glob('./tiny-imagenet-200/val/images/*.JPEG')\n",
    "    np.random.shuffle(val_images)\n",
    "\n",
    "    for i in range(len(val_images)):\n",
    "        if i < len(val_images) // 2:\n",
    "            copyfile(val_images[i], val_images[i].replace('images',\n",
    "                                                          'val_images'))\n",
    "        else:\n",
    "            copyfile(val_images[i], val_images[i].replace('images',\n",
    "                                                          'test_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6481d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path_train(path):\n",
    "    \"\"\"\n",
    "    Get the (class label, processed image) pair of the given image path. This\n",
    "    funciton uses python primitives, so you need to use tf.py_funciton wrapper.\n",
    "    This function uses global variables:\n",
    "\n",
    "        WIDTH(int): the width of the targeting image\n",
    "        HEIGHT(int): the height of the targeting iamge\n",
    "        NUM_CLASS(int): number of classes\n",
    "\n",
    "    Args:\n",
    "        path(string): path to an image file\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the class\n",
    "    path = path.numpy()\n",
    "    image_name = basename(path.decode('ascii'))\n",
    "    label_name = re.sub(r'(.+)_\\d+\\.JPEG', r'\\1', image_name)\n",
    "    label_index = tiny_class_dict[label_name]['index']\n",
    "\n",
    "    # Convert label to one-hot encoding\n",
    "    label = tf.one_hot(indices=[label_index], depth=NUM_CLASS)\n",
    "    label = tf.reshape(label, [NUM_CLASS])\n",
    "\n",
    "    # Read image and convert the image to [0, 1] range 3d tensor\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [WIDTH, HEIGHT])\n",
    "\n",
    "    return(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c43002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path_test(path):\n",
    "    \"\"\"\n",
    "    Get the (class label, processed image) pair of the given image path. This\n",
    "    funciton uses python primitives, so you need to use tf.py_funciton wrapper.\n",
    "    This function uses global variables:\n",
    "\n",
    "        WIDTH(int): the width of the targeting image\n",
    "        HEIGHT(int): the height of the targeting iamge\n",
    "        NUM_CLASS(int): number of classes\n",
    "\n",
    "    The filepath encoding for test images is different from training images.\n",
    "\n",
    "    Args:\n",
    "        path(string): path to an image file\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the class\n",
    "    path = path.numpy()\n",
    "    image_name = basename(path.decode('ascii'))\n",
    "    label_index = tiny_val_class_dict[image_name]['index']\n",
    "\n",
    "    # Convert label to one-hot encoding\n",
    "    label = tf.one_hot(indices=[label_index], depth=NUM_CLASS)\n",
    "    label = tf.reshape(label, [NUM_CLASS])\n",
    "\n",
    "    # Read image and convert the image to [0, 1] range 3d tensor\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [WIDTH, HEIGHT])\n",
    "\n",
    "    return(img, label)\n",
    "\n",
    "\n",
    "def prepare_for_training(dataset, batch_size=32, cache=True,\n",
    "                         shuffle_buffer_size=1000):\n",
    "\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "\n",
    "    # Only shuffle elements in the buffer size\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Pre featch batches in the background\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_for_testing(dataset, batch_size=32, cache=True):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "\n",
    "    # Pre featch batches in the background\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290ad747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(Model):\n",
    "    \"\"\"\n",
    "    Tiny VGG structure is adapted from http://cs231n.stanford.edu:\n",
    "        > This particular network is classifying CIFAR-10 images into one of 10\n",
    "        > classes and was trained with ConvNetJS. Its exact architecture is\n",
    "        > [conv-relu-conv-relu-pool]x3-fc-softmax, for a total of 17 layers and\n",
    "        > 7000 parameters. It uses 3x3 convolutions and 2x2 pooling regions.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=10):\n",
    "        super(TinyVGG, self).__init__()\n",
    "        self.conv_1_1 = Conv2D(filters, (3, 3), name='conv_1_1')\n",
    "        self.relu_1_1 = Activation('relu', name='relu_1_1')\n",
    "        self.conv_1_2 = Conv2D(filters, (3, 3), name='conv_1_2')\n",
    "        self.relu_1_2 = Activation('relu', name='relu_1_2')\n",
    "        self.max_pool_1 = MaxPool2D((2, 2), name='max_pool_1')\n",
    "\n",
    "        self.conv_2_1 = Conv2D(filters, (3, 3), name='conv_2_1')\n",
    "        self.relu_2_1 = Activation('relu', name='relu_2_1')\n",
    "        self.conv_2_2 = Conv2D(filters, (3, 3), name='conv_2_2')\n",
    "        self.relu_2_2 = Activation('relu', name='relu_2_2')\n",
    "        self.max_pool_2 = MaxPool2D((2, 2), name='max_pool_2')\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = Dense(NUM_CLASS, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv_1_1(x)\n",
    "        x = self.relu_1_1(x)\n",
    "        x = self.conv_1_2(x)\n",
    "        x = self.relu_1_2(x)\n",
    "        x = self.max_pool_1(x)\n",
    "\n",
    "        x = self.conv_2_1(x)\n",
    "        x = self.relu_2_1(x)\n",
    "        x = self.conv_2_2(x)\n",
    "        x = self.relu_2_2(x)\n",
    "        x = self.max_pool_2(x)\n",
    "\n",
    "        x = self.conv_3_1(x)\n",
    "        x = self.relu_3_1(x)\n",
    "        x = self.conv_3_2(x)\n",
    "        x = self.relu_3_2(x)\n",
    "        x = self.max_pool_3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d8e639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image_batch, label_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict\n",
    "        predictions = tiny_vgg(image_batch)\n",
    "\n",
    "        # Update gradient\n",
    "        loss = loss_object(label_batch, predictions)\n",
    "        gradients = tape.gradient(loss, tiny_vgg.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, tiny_vgg.trainable_variables))\n",
    "\n",
    "        train_mean_loss(loss)\n",
    "        train_accuracy(label_batch, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def vali_step(image_batch, label_batch):\n",
    "    predictions = tiny_vgg(image_batch)\n",
    "    vali_loss = loss_object(label_batch, predictions)\n",
    "\n",
    "    vali_mean_loss(vali_loss)\n",
    "    vali_accuracy(label_batch, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(image_batch, label_batch):\n",
    "    predictions = tiny_vgg(image_batch)\n",
    "    test_loss = loss_object(label_batch, predictions)\n",
    "\n",
    "    test_mean_loss(test_loss)\n",
    "    test_accuracy(label_batch, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83653008",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 64\n",
    "HEIGHT = 64\n",
    "EPOCHS = 100\n",
    "PATIENCE = 50\n",
    "LR = 0.001\n",
    "NUM_CLASS = 14\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c9c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/ec2-user/SageMaker/wikiart_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation dataset\n",
    "tiny_class_dict = load(open('./data/class_dict_10.json', 'r'))\n",
    "tiny_val_class_dict = load(open('./data/val_class_dict_10.json', 'r'))\n",
    "\n",
    "training_images = './data/class_10_train/*/images/*.JPEG'\n",
    "vali_images = './data/class_10_val/val_images/*.JPEG'\n",
    "test_images = './data/class_10_val/test_images/*.JPEG'\n",
    "\n",
    "# Create training dataset\n",
    "train_path_dataset = tf.data.Dataset.list_files(training_images)\n",
    "\n",
    "train_labeld_dataset = train_path_dataset.map(\n",
    "    lambda path: tf.py_function(\n",
    "        process_path_train,\n",
    "        [path],\n",
    "        [tf.float32, tf.float32]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create vali dataset\n",
    "vali_path_dataset = tf.data.Dataset.list_files(vali_images)\n",
    "\n",
    "vali_labeld_dataset = vali_path_dataset.map(\n",
    "    lambda path: tf.py_function(\n",
    "        process_path_test,\n",
    "        [path],\n",
    "        [tf.float32, tf.float32]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_path_dataset = tf.data.Dataset.list_files(test_images)\n",
    "\n",
    "test_labeld_dataset = test_path_dataset.map(\n",
    "    lambda path: tf.py_function(\n",
    "        process_path_test,\n",
    "        [path],\n",
    "        [tf.float32, tf.float32]\n",
    "    )\n",
    ")\n",
    "\n",
    "train_dataset = prepare_for_training(train_labeld_dataset,\n",
    "                                     batch_size=BATCH_SIZE)\n",
    "vali_dataset = prepare_for_training(vali_labeld_dataset,\n",
    "                                    batch_size=BATCH_SIZE)\n",
    "test_dataset = prepare_for_training(test_labeld_dataset,\n",
    "                                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "# tiny_vgg = TinyVGG()\n",
    "\n",
    "# Use Keras Sequential API instead, since it is easy to save the model\n",
    "filters = 10\n",
    "tiny_vgg = Sequential([\n",
    "    Conv2D(filters, (3, 3), input_shape=(64, 64, 3), name='conv_1_1'),\n",
    "    Activation('relu', name='relu_1_1'),\n",
    "    Conv2D(filters, (3, 3), name='conv_1_2'),\n",
    "    Activation('relu', name='relu_1_2'),\n",
    "    MaxPool2D((2, 2), name='max_pool_1'),\n",
    "\n",
    "    Conv2D(filters, (3, 3), name='conv_2_1'),\n",
    "    Activation('relu', name='relu_2_1'),\n",
    "    Conv2D(filters, (3, 3), name='conv_2_2'),\n",
    "    Activation('relu', name='relu_2_2'),\n",
    "    MaxPool2D((2, 2), name='max_pool_2'),\n",
    "\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(NUM_CLASS, activation='softmax', name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793479fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Compile\" the model with loss function and optimizer\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=LR)\n",
    "\n",
    "train_mean_loss = tf.keras.metrics.Mean(name='train_mean_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "vali_mean_loss = tf.keras.metrics.Mean(name='vali_mean_loss')\n",
    "vali_accuracy = tf.keras.metrics.CategoricalAccuracy(name='vali_accuracy')\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "no_improvement_epochs = 0\n",
    "best_vali_loss = np.inf\n",
    "start_time = time()\n",
    "print('Start training.\\n')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    for image_batch, label_batch in train_dataset:\n",
    "        train_step(image_batch, label_batch)\n",
    "\n",
    "    # Predict on the test dataset\n",
    "    for image_batch, label_batch in vali_dataset:\n",
    "        vali_step(image_batch, label_batch)\n",
    "\n",
    "    template = 'epoch: {}, train loss: {:.4f}, train accuracy: {:.4f}, '\n",
    "    template += 'vali loss: {:.4f}, vali accuracy: {:.4f}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_mean_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          vali_mean_loss.result(),\n",
    "                          vali_accuracy.result() * 100))\n",
    "\n",
    "    # Early stopping\n",
    "    if vali_mean_loss.result() < best_vali_loss:\n",
    "        no_improvement_epochs = 0\n",
    "        best_vali_loss = vali_mean_loss.result()\n",
    "        # Save the best model\n",
    "        tiny_vgg.save('trained_vgg_best.h5')\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= PATIENCE:\n",
    "        print('Early stopping at epoch = {}'.format(epoch))\n",
    "        break\n",
    "\n",
    "    # Reset evaluation metrics\n",
    "    train_mean_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    vali_mean_loss.reset_states()\n",
    "    vali_accuracy.reset_states()\n",
    "\n",
    "print('\\nFinished training, used {:.4f} mins.'.format((time() -\n",
    "                                                       start_time) / 60))\n",
    "# Save trained model\n",
    "tiny_vgg.save('trained_tiny_vgg.h5')\n",
    "tiny_vgg = tf.keras.models.load_model('trained_vgg_best.h5')\n",
    "\n",
    "# Test on hold-out test images\n",
    "test_mean_loss = tf.keras.metrics.Mean(name='test_mean_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "for image_batch, label_batch in test_dataset:\n",
    "    test_step(image_batch, label_batch)\n",
    "\n",
    "template = '\\ntest loss: {:.4f}, test accuracy: {:.4f}'\n",
    "print(template.format(test_mean_loss.result(),\n",
    "                      test_accuracy.result() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1392c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcbe02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
