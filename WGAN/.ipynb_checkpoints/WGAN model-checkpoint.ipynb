{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4187d6b4",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "91428639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee9ddf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa87e8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "db24fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4fb20a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c20ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9a608",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e395a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from models import *\n",
    "# from datasets import *\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e4e4f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c961f989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--sample_interval'], dest='sample_interval', nargs=None, const=None, default=100, type=<class 'int'>, choices=None, help='interval betwen image samples', metavar=None)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=2000, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1024, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"learning rate\") # was 0.00005\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=10, help=\"number of training steps for discriminator per iter\")\n",
    "parser.add_argument(\"--clip_value\", type=float, default=0.01, help=\"lower and upper clip value for disc. weights\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval betwen image samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ed07b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1024, channels=3, clip_value=0.01, img_size=64, latent_dim=100, lr=0.0002, n_cpu=8, n_critic=10, n_epochs=2000, sample_interval=100)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(\"\")\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "656b33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "189077b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e6f1f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e21821e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "373a7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ec2-user/SageMaker/genre-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ab431f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(opt.img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "image_datasets = datasets.ImageFolder(os.path.join(data_dir), data_transforms)\n",
    "# # Create training and validation dataloaders\n",
    "# dataloader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataloader = torch.utils.data.DataLoader(image_datasets, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d70c6aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 14981\n",
       "    Root location: /home/ec2-user/SageMaker/genre-224\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(64, 64), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b63cced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path('/home/ec2-user/SageMaker/portrait_landscape/1')\n",
    "corrupted = []\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open(os.path.join(path,filename)) # open the image file\n",
    "            img.verify() # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "#             pass\n",
    "            print('Bad file:', filename) # print out the names of corrupt files\n",
    "            corrupted.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e3bb3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images in corrupted:\n",
    "    os.remove(os.path.join(path,images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34884707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/2000] [Batch 0/15] [D loss: 0.013199] [G loss: 0.011938]\n",
      "[Epoch 0/2000] [Batch 10/15] [D loss: -140.359360] [G loss: -9.024948]\n",
      "[Epoch 1/2000] [Batch 0/15] [D loss: -175.444611] [G loss: -36.094719]\n",
      "[Epoch 1/2000] [Batch 10/15] [D loss: -186.070480] [G loss: -88.891174]\n",
      "[Epoch 2/2000] [Batch 0/15] [D loss: -118.922623] [G loss: -155.759979]\n",
      "[Epoch 2/2000] [Batch 10/15] [D loss: -45.034836] [G loss: -207.456299]\n",
      "[Epoch 3/2000] [Batch 0/15] [D loss: 16.427322] [G loss: -191.760590]\n",
      "[Epoch 3/2000] [Batch 10/15] [D loss: -32.049347] [G loss: -72.178139]\n",
      "[Epoch 4/2000] [Batch 0/15] [D loss: -48.712421] [G loss: -28.688244]\n",
      "[Epoch 4/2000] [Batch 10/15] [D loss: -89.197983] [G loss: 28.255409]\n",
      "[Epoch 5/2000] [Batch 0/15] [D loss: -93.053131] [G loss: 48.222977]\n",
      "[Epoch 5/2000] [Batch 10/15] [D loss: -114.225204] [G loss: 65.388237]\n",
      "[Epoch 6/2000] [Batch 0/15] [D loss: -112.659637] [G loss: 63.903999]\n",
      "[Epoch 6/2000] [Batch 10/15] [D loss: -115.170555] [G loss: 61.986237]\n",
      "[Epoch 7/2000] [Batch 0/15] [D loss: -116.239517] [G loss: 59.332809]\n",
      "[Epoch 7/2000] [Batch 10/15] [D loss: -108.424660] [G loss: 58.784439]\n",
      "[Epoch 8/2000] [Batch 0/15] [D loss: -105.502426] [G loss: 56.106762]\n",
      "[Epoch 8/2000] [Batch 10/15] [D loss: -109.588554] [G loss: 53.335720]\n",
      "[Epoch 9/2000] [Batch 0/15] [D loss: -105.411285] [G loss: 50.209320]\n",
      "[Epoch 9/2000] [Batch 10/15] [D loss: -99.571968] [G loss: 47.650784]\n",
      "[Epoch 10/2000] [Batch 0/15] [D loss: -90.800125] [G loss: 44.250946]\n",
      "[Epoch 10/2000] [Batch 10/15] [D loss: -93.382797] [G loss: 40.984383]\n",
      "[Epoch 11/2000] [Batch 0/15] [D loss: -87.025146] [G loss: 38.039806]\n",
      "[Epoch 11/2000] [Batch 10/15] [D loss: -82.694321] [G loss: 33.958481]\n",
      "[Epoch 12/2000] [Batch 0/15] [D loss: -78.114967] [G loss: 29.399973]\n",
      "[Epoch 12/2000] [Batch 10/15] [D loss: -72.142799] [G loss: 24.532593]\n",
      "[Epoch 13/2000] [Batch 0/15] [D loss: -90.409996] [G loss: 21.482250]\n",
      "[Epoch 13/2000] [Batch 10/15] [D loss: -81.708771] [G loss: 16.770042]\n",
      "[Epoch 14/2000] [Batch 0/15] [D loss: -83.553329] [G loss: 14.378309]\n",
      "[Epoch 14/2000] [Batch 10/15] [D loss: -95.468391] [G loss: 11.175957]\n",
      "[Epoch 15/2000] [Batch 0/15] [D loss: -83.568405] [G loss: 9.784021]\n",
      "[Epoch 15/2000] [Batch 10/15] [D loss: -80.379166] [G loss: 7.831443]\n",
      "[Epoch 16/2000] [Batch 0/15] [D loss: -89.243484] [G loss: 6.980425]\n",
      "[Epoch 16/2000] [Batch 10/15] [D loss: -95.889397] [G loss: 5.923433]\n",
      "[Epoch 17/2000] [Batch 0/15] [D loss: -94.581718] [G loss: 5.516922]\n",
      "[Epoch 17/2000] [Batch 10/15] [D loss: -80.179405] [G loss: 4.980152]\n",
      "[Epoch 18/2000] [Batch 0/15] [D loss: -92.532135] [G loss: 4.644147]\n",
      "[Epoch 18/2000] [Batch 10/15] [D loss: -88.967255] [G loss: 4.073766]\n",
      "[Epoch 19/2000] [Batch 0/15] [D loss: -84.622719] [G loss: 3.541266]\n",
      "[Epoch 19/2000] [Batch 10/15] [D loss: -108.788895] [G loss: 1.613775]\n",
      "[Epoch 20/2000] [Batch 0/15] [D loss: -93.232437] [G loss: -0.423077]\n",
      "[Epoch 20/2000] [Batch 10/15] [D loss: -113.085281] [G loss: -8.203886]\n",
      "[Epoch 21/2000] [Batch 0/15] [D loss: -115.971985] [G loss: -16.263592]\n",
      "[Epoch 21/2000] [Batch 10/15] [D loss: -131.270813] [G loss: -27.080322]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "batches_done = 0\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        def __getitem__(self, idx):\n",
    "            try:\n",
    "                img, label = load_img(idx)\n",
    "            except:\n",
    "                return None\n",
    "            return [img, label]\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z).detach()\n",
    "        # Adversarial loss\n",
    "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Clip weights of discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
    "\n",
    "        # Train the generator every n_critic iterations\n",
    "        if i % opt.n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "            # Adversarial loss\n",
    "            loss_G = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item())\n",
    "            )\n",
    "\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "        batches_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c17535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd36ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
